{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmaehler/Spoon-Knife/blob/main/notebooks/llmu/RAG_with_Chat_Embed_and_Rerank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ctaLvRUsfpj8",
      "metadata": {
        "id": "ctaLvRUsfpj8"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/cohere-ai/notebooks/blob/main/notebooks/llmu/RAG_with_Chat_Embed_and_Rerank.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61bac5b5",
      "metadata": {
        "id": "61bac5b5"
      },
      "source": [
        "# RAG with Chat, Embed, and Rerank\n",
        "\n",
        "This notebook shows how to build a RAG-powered chatbot with Cohere's Chat endpoint.  The chatbot can extract relevant information from external documents and produce verifiable, inline citations in its responses.\n",
        "\n",
        "Read the accompanying [article here](https://txt.cohere.com/rag-chatbot/).\n",
        "\n",
        "This application will use several Cohere API endpoints:\n",
        "\n",
        "- Chat: For handling the main logic of the chatbot, including turning a user message into queries, generating responses, and producing citations\n",
        "- Embed: For turning textual documents into their embeddings representation, later to be used in retrieval (we’ll use the latest, state-of-the-art Embed v3 model)\n",
        "- Rerank: For reranking the retrieved documents according to their relevance to a query\n",
        "\n",
        "The diagram below provides an overview of what we’ll build."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33327522",
      "metadata": {
        "id": "33327522"
      },
      "source": [
        "![Workflow](https://github.com/cohere-ai/notebooks/blob/main/notebooks/images/llmu/rag/rag-workflow-2.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6ab2d5d",
      "metadata": {
        "id": "f6ab2d5d"
      },
      "source": [
        "Here is a summary of the steps involved.\n",
        "\n",
        "Initial phase:\n",
        "- **Step 0**: Ingest the documents – get documents, chunk, embed, and index.\n",
        "\n",
        "For each user-chatbot interaction:\n",
        "- **Step 1**: Get the user message\n",
        "- **Step 2**: Call the Chat endpoint in query-generation mode\n",
        "- If at least one query is generated\n",
        "  - **Step 3**: Retrieve and rerank relevant documents\n",
        "  - **Step 4**: Call the Chat endpoint in document mode to generate a grounded response with citations\n",
        "- If no query is generated\n",
        "  - **Step 4**: Call the Chat endpoint in normal mode to generate a response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TWyo_5WoNUM-",
      "metadata": {
        "id": "TWyo_5WoNUM-"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "5pLAhQmTOKiV",
      "metadata": {
        "id": "5pLAhQmTOKiV",
        "outputId": "163ded1f-599f-4b78-a05c-6a83c691280b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install cohere hnswlib unstructured -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "f3a03a57",
      "metadata": {
        "id": "f3a03a57",
        "outputId": "2e8a7f3d-a819-4024-c5fc-5a22bcc880e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import cohere\n",
        "import uuid\n",
        "import hnswlib\n",
        "from typing import List, Dict\n",
        "from unstructured.partition.html import partition_html\n",
        "from unstructured.chunking.title import chunk_by_title\n",
        "\n",
        "co = cohere.Client(\"eNcFOm1uF4d3GLIKdF1HoyhxSIiQResbJeFIswh4\") # Get your API key here: https://dashboard.cohere.com/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "Dx1cncziCWBB",
      "metadata": {
        "id": "Dx1cncziCWBB",
        "outputId": "68a9fad8-7f29-4a93-c748-2034db8d2a46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Enable text wrapping in Google Colab\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d34e4b7",
      "metadata": {
        "id": "9d34e4b7"
      },
      "source": [
        "# Create a vector store for ingestion and retrieval\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "588ed6d0",
      "metadata": {
        "id": "588ed6d0"
      },
      "source": [
        "![RAG components - Vectorstore](https://github.com/cohere-ai/notebooks/blob/main/notebooks/images/llmu/rag/rag-components-vectorstore.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f7e7d1c",
      "metadata": {
        "id": "2f7e7d1c"
      },
      "source": [
        "\n",
        "First, we define the list of documents we want to ingest and make available for retrieval. As an example, we'll use the contents from the first module of Cohere's *LLM University: What are Large Language Models?*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "3dca4a88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3dca4a88",
        "outputId": "2309973d-1038-4432-f831-89932e06c808"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "raw_documents = [\n",
        "    {\n",
        "        \"title\": \"Crafting Effective Prompts\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/crafting-effective-prompts\"},\n",
        "    {\n",
        "        \"title\": \"Advanced Prompt Engineering Techniques\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\"},\n",
        "    {\n",
        "        \"title\": \"Prompt Truncation\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/prompt-truncation\"},\n",
        "    {\n",
        "        \"title\": \"Preambles\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/preambles\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e2a8968",
      "metadata": {
        "id": "5e2a8968"
      },
      "source": [
        "Usually the number of documents for practical applications is vast, and so we'll need to be able to search documents efficiently.  This involves breaking the documents into chunks, generating embeddings, and indexing the embeddings, as shown in the image below.  \n",
        "\n",
        "We implement this in the `Vectorstore` class below, which takes the `raw_documents` list as input.  Three methods are immediately called when creating an object of the `Vectorstore` class:\n",
        "\n",
        "\n",
        "`load_and_chunk()`  \n",
        "This method uses the `partition_html()` method from the `unstructured` library to load the documents from URL and break them into smaller chunks.  Each chunk is turned into a dictionary object with three fields:\n",
        "- `title` - the web page’s title,\n",
        "- `text` - the textual content of the chunk, and\n",
        "- `url` - the web page’s URL.  \n",
        "  \n",
        "  \n",
        "`embed()`  \n",
        "This method uses Cohere's `embed-english-v3.0` model to generate embeddings of the chunked documents.  Since our documents will be used for retrieval, we set `input_type=\"search_document\"`.  We send the documents to the Embed endpoint in batches, because the endpoint has a limit of 96 documents per call.\n",
        "\n",
        "`index()`  \n",
        "This method uses the `hsnwlib` package to index the document chunk embeddings.  This will ensure efficient similarity search during retrieval.  Note that `hnswlib` uses a vector library, and we have chosen it for its simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "c28df755",
      "metadata": {
        "id": "c28df755",
        "outputId": "e4c50d38-581a-4a09-ad0b-65b4d4deeb35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import List, Dict\n",
        "import hnswlib\n",
        "def partition_html(url: str) -> List[Dict[str, str]]:\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    elements = []\n",
        "    for tag in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):\n",
        "        elements.append({\n",
        "            'tag': tag.name,\n",
        "            'text': tag.get_text(strip=True)\n",
        "        })\n",
        "    return elements\n",
        "def chunk_by_title(elements: List[Dict[str, str]]) -> List[str]:\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for element in elements:\n",
        "        if element['tag'].startswith('h'):\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            current_chunk = element['text'] + \"\\n\"\n",
        "        else:\n",
        "            current_chunk += element['text'] + \"\\n\"\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "class Vectorstore:\n",
        "    def __init__(self, raw_documents: List[Dict[str, str]]):\n",
        "        self.raw_documents = raw_documents\n",
        "        self.docs = []\n",
        "        self.docs_embs = []\n",
        "        self.retrieve_top_k = 10\n",
        "        self.rerank_top_k = 3\n",
        "        self.load_and_chunk()\n",
        "        self.embed()\n",
        "        self.index()\n",
        "    def load_and_chunk(self) -> None:\n",
        "        print(\"Loading documents...\")\n",
        "        for raw_document in self.raw_documents:\n",
        "            elements = partition_html(url=raw_document[\"url\"])\n",
        "            chunks = chunk_by_title(elements)\n",
        "            for chunk in chunks:\n",
        "                self.docs.append(\n",
        "                    {\n",
        "                        \"title\": raw_document[\"title\"],\n",
        "                        \"text\": chunk,\n",
        "                        \"url\": raw_document[\"url\"],\n",
        "                    }\n",
        "                )\n",
        "    def embed(self) -> None:\n",
        "        print(\"Embedding document chunks...\")\n",
        "        batch_size = 90\n",
        "        self.docs_len = len(self.docs)\n",
        "        for i in range(0, self.docs_len, batch_size):\n",
        "            batch = self.docs[i : min(i + batch_size, self.docs_len)]\n",
        "            texts = [item[\"text\"] for item in batch]\n",
        "            docs_embs_batch = co.embed(\n",
        "                texts=texts, model=\"embed-english-v3.0\", input_type=\"search_document\"\n",
        "            ).embeddings\n",
        "            self.docs_embs.extend(docs_embs_batch)\n",
        "    def index(self) -> None:\n",
        "        print(\"Indexing document chunks...\")\n",
        "        self.idx = hnswlib.Index(space=\"ip\", dim=1024)\n",
        "        self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64)\n",
        "        self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs))))\n",
        "        print(f\"Indexing complete with {self.idx.get_current_count()} document chunks.\")\n",
        "    def retrieve(self, query: str) -> List[Dict[str, str]]:\n",
        "        query_emb = co.embed(\n",
        "            texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\"\n",
        "        ).embeddings\n",
        "        doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
        "        rank_fields = [\"title\", \"text\"]\n",
        "        docs_to_rerank = [self.docs[doc_id] for doc_id in doc_ids]\n",
        "        rerank_results = co.rerank(\n",
        "            query=query,\n",
        "            documents=docs_to_rerank,\n",
        "            top_n=self.rerank_top_k,\n",
        "            model=\"rerank-english-v3.0\",\n",
        "            rank_fields=rank_fields\n",
        "        )\n",
        "        doc_ids_reranked = [doc_ids[result.index] for result in rerank_results.results]\n",
        "        docs_retrieved = []\n",
        "        for doc_id in doc_ids_reranked:\n",
        "            docs_retrieved.append(\n",
        "                {\n",
        "                    \"title\": self.docs[doc_id][\"title\"],\n",
        "                    \"text\": self.docs[doc_id][\"text\"],\n",
        "                    \"url\": self.docs[doc_id][\"url\"],\n",
        "                }\n",
        "            )\n",
        "        return docs_retrieved\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1bf5d85",
      "metadata": {
        "id": "e1bf5d85"
      },
      "source": [
        "In the code cell below, we initialize an instance of the `Vectorstore` class and pass in the `raw_documents` list as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "4643e630",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "4643e630",
        "outputId": "9c0a223a-3972-4cab-ed71-76d434b0f342"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading documents...\n",
            "Embedding document chunks...\n",
            "Indexing document chunks...\n",
            "Indexing complete with 30 document chunks.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create an instance of the Vectorstore class with the given sources\n",
        "vectorstore = Vectorstore(raw_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61928287",
      "metadata": {
        "id": "61928287"
      },
      "source": [
        "The `Vectorstore` class also has a `retrieve()` method, which we'll use to retrieve relevant document chunks given a query (as in Step 3 in the diagram shared at the beginning of this notebook).  This method has two components: (1) dense retrieval, and (2) reranking.\n",
        "\n",
        "### Dense retrieval\n",
        "\n",
        "First, we embed the query using the same `embed-english-v3.0` model we used to embed the document chunks, but this time we set `input_type=\"search_query\"`.\n",
        "\n",
        "Search is performed by the `knn_query()` method from the `hnswlib` library. Given a query, it returns the document chunks most similar to the query. We can define the number of document chunks to return using the attribute `self.retrieve_top_k=10`.\n",
        "\n",
        "### Reranking\n",
        "\n",
        "After semantic search, we implement a reranking step.  While our semantic search component is already highly capable of retrieving relevant sources, the [Rerank endpoint](https://cohere.com/rerank) provides an additional boost to the quality of the search results, especially for complex and domain-specific queries. It takes the search results and sorts them according to their relevance to the query.\n",
        "\n",
        "We call the Rerank endpoint with the `co.rerank()` method and define the number of top reranked document chunks to retrieve using the attribute `self.rerank_top_k=3`.  The model we use is `rerank-english-v2.0`.  \n",
        "\n",
        "This method returns the top retrieved document chunks `chunks_retrieved` so that they can be passed to the chatbot.\n",
        "\n",
        "In the code cell below, we check the document chunks that are retrieved for the query `\"multi-head attention definition\"`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OwozNf_uPEyX",
      "metadata": {
        "id": "OwozNf_uPEyX"
      },
      "source": [
        "## Test Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "82617b91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "82617b91",
        "outputId": "f76d53e8-4fea-4e19-b483-aad049654b4f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Advanced Prompt Engineering Techniques',\n",
              "  'text': 'Few-shot Prompting\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\nIn addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.\\nTo improve the above question, we can include several positive and negative examples in random order from the LegalBench training set as follows:\\nThe model continues to answer correctly, and now it also backs up the answer with a clear explanation.',\n",
              "  'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'},\n",
              " {'title': 'Crafting Effective Prompts',\n",
              "  'text': 'Incorporating Example Outputs\\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.',\n",
              "  'url': 'https://docs.cohere.com/docs/crafting-effective-prompts'},\n",
              " {'title': 'Crafting Effective Prompts',\n",
              "  'text': 'Crafting Effective Prompts\\nThe most effective prompts are those that are clear, concise, specific, and include examples of exactly what a response should look like. In this chapter, we will cover several strategies and tactics to get the most effective responses from the Command family of models. We will cover formatting and delimiters, context, using examples, structured output, do vs. do not do, length control, begin the completion yourself, and task splitting. We will highlight best practices as a user crafting prompts in the Cohere playground, as well as through the API.',\n",
              "  'url': 'https://docs.cohere.com/docs/crafting-effective-prompts'}]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "vectorstore.retrieve(\"Prompting by giving examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bae81baa",
      "metadata": {
        "id": "bae81baa"
      },
      "source": [
        "# Run chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c88017f4",
      "metadata": {
        "id": "c88017f4"
      },
      "source": [
        "![RAG components - Chatbot](https://github.com/cohere-ai/notebooks/blob/main/notebooks/images/llmu/rag/rag-components-chatbot.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e69fbca9",
      "metadata": {
        "id": "e69fbca9"
      },
      "source": [
        "We can now run the chatbot. For this, we create a generate_chat function which includes the RAG components:\n",
        "- For each user message, we use the endpoint’s search query generation feature to turn the message into one or more queries that are optimized for retrieval. The endpoint can even return no query, which means that a user message can be responded to directly without retrieval. This is done by calling the Chat endpoint with the search_queries_only parameter and setting it as True.\n",
        "- If there is no search query generated, we call the Chat endpoint to generate a response directly. If there is at least one, we call the retrieve method from the Vectorstore instance to retrieve the most relevant documents to each query.\n",
        "- Finally, all the results from all queries are appended to a list and passed to the Chat endpoint for response generation.\n",
        "- We print the response, together with the citations and the list of document chunks cited, for easy reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "d2c15a1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "d2c15a1f",
        "outputId": "7f42f3c2-e966-4c4f-ea30-f966b6dae7ab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def run_chatbot(message, chat_history=None):\n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "\n",
        "    # Generate search queries, if any\n",
        "    response = co.chat(message=message,\n",
        "                        model=\"command-r-plus\",\n",
        "                        search_queries_only=True,\n",
        "                        chat_history=chat_history)\n",
        "\n",
        "    search_queries = []\n",
        "    for query in response.search_queries:\n",
        "        search_queries.append(query.text)\n",
        "\n",
        "    # If there are search queries, retrieve the documents\n",
        "    if search_queries:\n",
        "        print(\"Retrieving information...\", end=\"\")\n",
        "\n",
        "        # Retrieve document chunks for each query\n",
        "        documents = []\n",
        "        for query in search_queries:\n",
        "            documents.extend(vectorstore.retrieve(query))\n",
        "\n",
        "        # Use document chunks to respond\n",
        "        response = co.chat_stream(\n",
        "            message=message,\n",
        "            model=\"command-r-plus\",\n",
        "            documents=documents,\n",
        "            chat_history=chat_history,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        response = co.chat_stream(\n",
        "            message=message,\n",
        "            model=\"command-r-plus\",\n",
        "            chat_history=chat_history,\n",
        "        )\n",
        "\n",
        "    # Print the chatbot response, citations, and documents\n",
        "    chatbot_response = \"\"\n",
        "    print(\"\\nChatbot:\")\n",
        "\n",
        "    for event in response:\n",
        "        if event.event_type == \"text-generation\":\n",
        "            print(event.text, end=\"\")\n",
        "            chatbot_response += event.text\n",
        "        if event.event_type == \"stream-end\":\n",
        "            if event.response.citations:\n",
        "                print(\"\\n\\nCITATIONS:\")\n",
        "                for citation in event.response.citations:\n",
        "                    print(citation)\n",
        "            if event.response.documents:\n",
        "                print(\"\\nCITED DOCUMENTS:\")\n",
        "                for document in event.response.documents:\n",
        "                    print(document)\n",
        "            # Update the chat history for the next turn\n",
        "            chat_history = event.response.chat_history\n",
        "\n",
        "    return chat_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd03d5f",
      "metadata": {
        "id": "dfd03d5f"
      },
      "source": [
        "Here is a sample conversation consisting of a few turns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "cc071a0b",
      "metadata": {
        "id": "cc071a0b",
        "outputId": "d038127c-bd37-4c16-e36b-5b66ef2d533b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving information...\n",
            "Chatbot:\n",
            "Prompt engineering is a technique used to improve the quality of an LLM's completions. This can be done by giving clear and unambiguous instructions, few-shot prompting, chain-of-thought techniques, and prompt chaining. Few-shot prompting provides the model with examples of the task being performed and relevant and diverse examples to steer the LLM toward a high-quality solution. Prompt chaining forces the model to slow down and break a task into constituent parts.\n",
            "\n",
            "CITATIONS:\n",
            "start=42 end=86 text=\"improve the quality of an LLM's completions.\" document_ids=['doc_0']\n",
            "start=107 end=148 text='giving clear and unambiguous instructions' document_ids=['doc_0']\n",
            "start=150 end=168 text='few-shot prompting' document_ids=['doc_0', 'doc_1']\n",
            "start=170 end=197 text='chain-of-thought techniques' document_ids=['doc_0']\n",
            "start=203 end=219 text='prompt chaining.' document_ids=['doc_0', 'doc_2']\n",
            "start=263 end=299 text='examples of the task being performed' document_ids=['doc_1']\n",
            "start=304 end=333 text='relevant and diverse examples' document_ids=['doc_1']\n",
            "start=337 end=382 text='steer the LLM toward a high-quality solution.' document_ids=['doc_1']\n",
            "start=419 end=469 text='slow down and break a task into constituent parts.' document_ids=['doc_2']\n",
            "\n",
            "CITED DOCUMENTS:\n",
            "{'id': 'doc_0', 'text': 'Advanced Prompt Engineering Techniques\\nThe previous chapter discussed general rules and heuristics to follow for successfully prompting the Command family of models. Here, we will discuss specific advanced prompt engineering techniques that can in many cases vastly improve the quality of the model’s completions. These include how to give clear and unambiguous instructions, few-shot prompting, chain-of-thought (CoT) techniques, and prompt chaining.\\nAs we develop these techniques, we will work through an example where our aim is to improve a prompt from theLegalBench“hearsay” task. The task asks an LLM to determine whether a particular piece of evidence qualifies as hearsay. Hearsay is an out-of-court statement introduced to prove the truth of the matter asserted. For example, the following two samples provide examples of statements that are, and are not, hearsay.\\nBefore we apply any specific prompting techniques, we can see that simply prompting the model with the direct question results in too much unwanted and ambiguous information:\\nUsing the Chat API, we could do the following:\\nThe answer returned with this method is unfortunately wrong. The correct answer is “Yes” (non-verbal hearsay). Without a definition of the task or other additional context the model can sometimes make an incorrect assertion and then attempt to reconcile what has already been generated.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}\n",
            "{'id': 'doc_1', 'text': 'Few-shot Prompting\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\nIn addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.\\nTo improve the above question, we can include several positive and negative examples in random order from the LegalBench training set as follows:\\nThe model continues to answer correctly, and now it also backs up the answer with a clear explanation.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}\n",
            "{'id': 'doc_2', 'text': 'Prompt Chaining\\nFinally, prompt chaining can explicitly force a model to slow down and break a task into constituent parts. As explained in the previous chapter, task splitting can be an effective technique to improve the quality of completions. However, an LLM will sometimes try to jump to the answer immediately. Further, one can include more complex instructions without as high of a chance of them being lost in the information overload.\\nFor example, instead of asking the model to “work through the problem step by step” before answering (which in certain cases LLMs can forget to do), we can first ask for an analysis of the situation, then ask for a simple “yes” or “no” answer.\\nThe issue was analyzed correctly in the above completion, but we are seeking a clear “Yes” or “No” answer that a downstream task can easily ingest. Therefore, we chain the completion of the first prompt with a second prompt:\\nChaining prompts together allows us to use the first prompt to focus on the analysis, and the second to properly extract the information in a single-word response.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}\n"
          ]
        }
      ],
      "source": [
        "# Turn # 1\n",
        "chat_history = run_chatbot(\"Hello, what is prompt engineering?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "b84bfdb0",
      "metadata": {
        "id": "b84bfdb0",
        "outputId": "67cff551-561c-4a19-b664-c258acf9b902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving information...\n",
            "Chatbot:\n",
            "Few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. This can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\n",
            "\n",
            "CITATIONS:\n",
            "start=39 end=97 text='provides a model with examples of the task being performed' document_ids=['doc_0']\n",
            "start=105 end=149 text='asking the specific question to be answered.' document_ids=['doc_0']\n",
            "start=159 end=203 text='steer the LLM toward a high-quality solution' document_ids=['doc_0']\n",
            "start=223 end=267 text='relevant and diverse examples in the prompt.' document_ids=['doc_0']\n",
            "start=282 end=342 text='condition the model to the expected response type and style.' document_ids=['doc_0']\n",
            "\n",
            "CITED DOCUMENTS:\n",
            "{'id': 'doc_0', 'text': 'Few-shot Prompting\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\nIn addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.\\nTo improve the above question, we can include several positive and negative examples in random order from the LegalBench training set as follows:\\nThe model continues to answer correctly, and now it also backs up the answer with a clear explanation.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}\n"
          ]
        }
      ],
      "source": [
        "# Turn # 2\n",
        "chat_history = run_chatbot(\"What is few-shot prompting?\", chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "d60ce722",
      "metadata": {
        "id": "d60ce722",
        "outputId": "58041c0c-bbc8-4ef8-e772-504f69972550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving information...\n",
            "Chatbot:\n",
            "Few-shot prompting helps steer the LLM towards a high-quality solution by providing it with a few relevant and diverse examples. These examples condition the model to the expected response type and style. Including both positive and negative examples, with an indication of why the negative examples are wrong, helps the LLM to learn to distinguish between correct and incorrect responses. This technique also helps to avoid the LLM picking up on irrelevant patterns by randomising the order of the examples.\n",
            "\n",
            "CITATIONS:\n",
            "start=25 end=70 text='steer the LLM towards a high-quality solution' document_ids=['doc_0']\n",
            "start=74 end=128 text='providing it with a few relevant and diverse examples.' document_ids=['doc_0']\n",
            "start=144 end=204 text='condition the model to the expected response type and style.' document_ids=['doc_0']\n",
            "start=220 end=250 text='positive and negative examples' document_ids=['doc_0']\n",
            "start=260 end=309 text='indication of why the negative examples are wrong' document_ids=['doc_0']\n",
            "start=328 end=389 text='learn to distinguish between correct and incorrect responses.' document_ids=['doc_0']\n",
            "start=419 end=466 text='avoid the LLM picking up on irrelevant patterns' document_ids=['doc_0']\n",
            "start=470 end=508 text='randomising the order of the examples.' document_ids=['doc_0']\n",
            "\n",
            "CITED DOCUMENTS:\n",
            "{'id': 'doc_0', 'text': 'Few-shot Prompting\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\nIn addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.\\nTo improve the above question, we can include several positive and negative examples in random order from the LegalBench training set as follows:\\nThe model continues to answer correctly, and now it also backs up the answer with a clear explanation.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}\n"
          ]
        }
      ],
      "source": [
        "# Turn # 3\n",
        "chat_history = run_chatbot(\"How would the latter help?\", chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "c9faca1e",
      "metadata": {
        "id": "c9faca1e",
        "outputId": "c1802e49-fabe-4c3e-e0ea-cbb0e38f9bcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving information...\n",
            "Chatbot:\n",
            "I'm sorry, I do not have access to information about 5G networks. Can I help you with anything else?"
          ]
        }
      ],
      "source": [
        "# Turn # 4\n",
        "chat_history = run_chatbot(\"What do you know about 5G networks?\", chat_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0bf24de",
      "metadata": {
        "id": "d0bf24de"
      },
      "source": [
        "There are a few observations worth pointing out:\n",
        "\n",
        "- Direct response: For user messages that don’t require retrieval (“Hello, I have a question”), the chatbot responds directly without requiring retrieval.\n",
        "- Citation generation: For responses that do require retrieval (\"What's the difference between zero-shot and few-shot prompting\"), the endpoint returns the response together with the citations. These are fine-grained citations, which means they refer to specific spans of the generated text.\n",
        "- State management: The endpoint maintains the state of the conversation via the chat_history parameter, for example, by correctly responding to a vague user message such as \"How would the latter help?\"\n",
        "- Response synthesis: The model can decide if none of the retrieved documents provide the necessary information to answer a user message. For example, when asked the question, “What do you know about 5G networks”, the chatbot retrieves external information from the index. However, it doesn’t use any of the information in its response as none of it is relevant to the question."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "154c95a2",
      "metadata": {
        "id": "154c95a2"
      },
      "source": [
        "Here are the contents of the chat history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "3281f3b7",
      "metadata": {
        "id": "3281f3b7",
        "outputId": "141e1d73-57be-4a28-9634-924bdcf68659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat history:\n",
            "message='Hello, I have a question' tool_calls=None role='USER' \n",
            "\n",
            "message='Of course! I am here to help. Please go ahead with your question, and I will do my best to provide a helpful response.' tool_calls=None role='CHATBOT' \n",
            "\n",
            "message=\"What's the difference between zero-shot and few-shot prompting\" tool_calls=None role='USER' \n",
            "\n",
            "message='Zero-shot prompting is when a model is asked to perform a task without being given any examples of how to do so. On the other hand, few-shot prompting provides the model with a small number of relevant examples before asking the specific question to be answered. These examples can be both positive and negative, with an indication of why the negative examples are wrong, to help the model learn to distinguish between correct and incorrect responses.' tool_calls=None role='CHATBOT' \n",
            "\n",
            "message='How would the latter help?' tool_calls=None role='USER' \n",
            "\n",
            "message='Few-shot prompting helps steer the LLM towards a high-quality solution by providing it with a few relevant and diverse examples. These examples condition the model to the expected response type and style. Including both positive and negative examples, with an indication of why the negative examples are wrong, helps the LLM to learn to distinguish between correct and incorrect responses. This technique also helps to avoid the LLM picking up on irrelevant patterns by randomising the order of the examples.' tool_calls=None role='CHATBOT' \n",
            "\n",
            "message='What do you know about 5G networks?' tool_calls=None role='USER' \n",
            "\n",
            "message=\"I'm sorry, I do not have access to information about 5G networks. Can I help you with anything else?\" tool_calls=None role='CHATBOT' \n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"Chat history:\")\n",
        "for c in chat_history:\n",
        "    print(c, \"\\n\")\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}